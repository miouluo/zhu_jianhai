{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![](1.png)\n",
    "## 赛题名称\n",
    "- Optiver - Trading at the Close\n",
    "## 赛题链接\n",
    "- https://www.kaggle.com/competitions/optiver-trading-at-the-close\n",
    "# 赛题背景\n",
    "证券交易所是快节奏、高风险的环境，每一秒都很重要。随着交易日接近尾声，强度不断升级，在关键的最后十分钟达到顶峰。这些时刻通常以波动加剧和价格快速波动为特征，在塑造当今全球经济叙事方面发挥着关键作用。\n",
    "\n",
    "纳斯达克证券交易所的每个交易日都以纳斯达克收盘交叉拍卖结束。此过程确定了在交易所上市的证券的官方收盘价。这些收盘价是投资者、分析师和其他市场参与者评估个别证券和整个市场表现的关键指标。\n",
    "\n",
    "在这个复杂的金融环境中，Optiver是全球领先的电子做市商。在技术创新的推动下，Optiver交易各种金融工具，如衍生品，现金股票，ETF，债券和外币，为全球主要交易所的数千种此类工具提供具有竞争力的双边价格。\n",
    "\n",
    "在纳斯达克交易所交易时段的最后十分钟，像Optiver这样的做市商将传统的订单簿数据与拍卖簿数据合并。这种整合来自两个来源的信息的能力对于为所有市场参与者提供最优惠的价格至关重要。\n",
    "\n",
    "# 赛题任务\n",
    "在本次比赛中，您面临的挑战是开发一个模型，该模型能够使用订单簿和股票收盘价中的数据预测数百只纳斯达克上市股票的<font color='yellow'>收盘价走势</font>。拍卖信息可用于调整价格、评估供需动态以及识别交易机会。\n",
    "\n",
    "# 评价指标\n",
    "根据预测回报和观测目标之间的平均绝对误差 <font color='yellow'>（MAE）</font> 评估提交。公式由下式给出：\n",
    "MAE = (1/n) * Σ(|y_i - y_hat_i|)\n",
    "其中，n表示样本数量，y_i表示真实值，y_hat_i表示预测值。Σ表示求和符号，| |表示取绝对值。   **<font color='yellow'>越低越好</font>**\n",
    "\n",
    "# 数据说明\n",
    "[训练集/测试集].csv 拍卖数据。测试集将通过API提供。\n",
    "- stock_id - 股票的唯一标识符。不会在所有时间段都出现所有的股票ID。<font color='yellow'>分析不同股票之间的行为和差异</font>\n",
    "\n",
    "- date_id - 日期的唯一标识符。日期ID是顺序一致的，贯穿所有股票。<font color='yellow'>可用于将数据按日期进行分组和分析。</font>\n",
    "\n",
    "- imbalance_size - 在当前参考价格下无法配对的金额(美元)。<font color='yellow'>不平衡大小，表示买卖不平衡的大小差异。可用于分析市场不平衡程度。</font>\n",
    "\n",
    "- imbalance_buy_sell_flag - 反映拍卖不平衡方向的指标。<font color='yellow'>可用于分析市场趋势。</font>\n",
    "    - 买方不平衡；1\n",
    "\n",
    "    - 卖方不平衡；-1\n",
    "\n",
    "    - 无不平衡；0\n",
    "\n",
    "- reference_price - 配对股份最大化,不平衡最小化,且与买卖盘中点距离最小化的价格,依此顺序。可以视为受最佳买入和卖出价格限定的接近价格。<font color='yellow'>通常是开盘价，可用于分析价格相对于开盘价的变化。</font>\n",
    "\n",
    "- matched_size - 在当前参考价格下可以配对的金额(美元)。<font color='yellow'>表示已匹配的订单的大小。可用于分析市场活动和交易量。</font>\n",
    "\n",
    "- far_price - 仅基于拍卖利益可以配对最多股份的交叉价格。该计算不包括连续市场订单。\n",
    "\n",
    "- near_price - 基于拍卖和连续市场订单可以配对最多股份的交叉价格。\n",
    "\n",
    "- [bid/ask]_price - 非拍卖订单簿中最具竞争力的买入/卖出价格。<font color='yellow'>买单价格和买单大小，表示当前的买单信息，可用于分析市场深度和支撑水平。</font>\n",
    "\n",
    "- [bid/ask]_size - 非拍卖订单簿最具竞争力的买入/卖出金额。<font color='yellow'>买单价格和买单大小，表示当前的买单信息，可用于分析市场深度和支撑水平。</font>\n",
    "\n",
    "- wap - 非拍卖订单簿的加权平均价格。<font color='yellow'>是买卖价格的交易量加权平均值，可用于估计当前市场价格。</font>\n",
    "\n",
    "- seconds_in_bucket - 从当日收盘拍卖开始经过的秒数，总是从0开始。<font color='yellow'>每个数据点在时间窗口内的秒数，可用于分析数据点的时间分布和行为。</font>\n",
    "\n",
    "- target - 股票wap在未来60秒内的价格变动减去合成指数在未来60秒内的价格变动。\n",
    "\n",
    "------------------------------\n",
    "\n",
    "# Data EDA\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86a6d4e8df41d329"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd  # 数据处理\n",
    "import numpy as np  # 科学运算\n",
    "import matplotlib.pyplot as plt # 绘图\n",
    "import seaborn as sns # 绘图\n",
    "import warnings # 无视警告\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 配置文件\n",
    "class CONFIG():\n",
    "    train_path = '../train.csv'\n",
    "    test_path = '../example_test_files/test.csv'\n",
    "    \n",
    "# 加载数据\n",
    "train = pd.read_csv(CONFIG.train_path).drop(['row_id'], axis=1)\n",
    "test = pd.read_csv(CONFIG.test_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:41:49.315550900Z",
     "start_time": "2023-10-25T14:41:40.522205900Z"
    }
   },
   "id": "c7315307c07e44c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ece8bf41208480"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cb5a5d6165c56ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.describe().T"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8f8b3b71d3defec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "结论：\n",
    "- stock_id：范围从0到199，近似平均值为99.29。不存在空值。\n",
    "- date_id：范围从0到480，近似平均值为241.51。此外，不存在空值。\n",
    "- seconds_in_bucket：以秒为单位的时间范围从0到540，平均约为270秒。\n",
    "- imbalance_size：范围很广，从0到大约29.8亿。这可能表明存在异常值。\n",
    "- imbalance_buy_sell_flag：这是一个二进制指示符，范围从-1到1。接近零的平均值表明买卖之间存在大致平衡。\n",
    "- reference_price：平均而言，参考价格接近1，最低0.9353，最高1.0775。\n",
    "- matched_size：与imbalance_size类似，它的范围很广，可能包含异常值。\n",
    "- far_price和near_price：两者都接近1，但范围不同，这可能是进一步探索的有趣之处。\n",
    "- bid_price和ask_price：两者都非常接近1，并且具有相似的方差。\n",
    "- bid_size和ask_size：两者都有相似的均值和方差，表明订单簿在出价和要价方面可能相对平衡。\n",
    "- wap（加权平均价格）：平均值接近1，类似于bid_Price和ask_Price。\n",
    "- target：这是我们预测的目标。其平均值约为-0.0476，标准差较大，表明变异性较大。\n",
    "- time_id：范围从0到26454，近似平均值为13310。\n",
    "- Null值：列imbalance_size、reference_price、matched_size、far_price和near_price、bid_price、ask_price，wap和target具有一些需要处理的Null值。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "312dd953eb3f1eeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 缺失值检查\n",
    "train.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "feee0c1df3eeda0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "imbalance_size、reference_price、matched_size、bid_price、ask_price和wap：这些列各缺少220个值。由于这些列中缺失值的数量相同，因此很可能是同一行中缺失了这些值。这将需要通过插补或删除这些行来解决。\n",
    "\n",
    "far_price：此列有2894342个缺失值，明显高于其他列。需要特别注意处理这些缺失值，可能通过插补或使用不同的特征工程策略。\n",
    "\n",
    "near_price：此列缺少2857180个值，也明显高于其他列，但少于far_price。与far_price类似的策略也可以应用于此。\n",
    "\n",
    "target:缺少88个值。假设这是目标变量，则最有可能需要从训练集中删除缺少目标值的行。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b5b0e1d4e6542a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 同类股票的平均WAP\n",
    "# 计算每只股票的WAP平均值和标准差\n",
    "grouped_stocks = train.groupby('stock_id')['wap'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# 根据平均WAP和标准差对股票进行排序\n",
    "grouped_stocks.sort_values(by=['mean', 'std'], inplace=True)\n",
    "\n",
    "# 创建由10只类似股票组成的小组\n",
    "grouped_stocks['group'] = grouped_stocks.index // 10\n",
    "\n",
    "# Plotting the mean WAP for the first 10 groups\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.boxplot(x='group', y='mean', data=grouped_stocks[grouped_stocks['group'] < 10])\n",
    "plt.title('Mean WAP for Groups of 10 Similar Stocks')\n",
    "plt.xlabel('Group')\n",
    "plt.ylabel('Mean WAP')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17251b9a02ae0960"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "grouped_stocks.head(20).set_index('stock_id')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1e698a279a4a219"
  },
  {
   "cell_type": "markdown",
   "source": [
    "结论：\n",
    "- WAP效果很好: 这些股票组的平均WAP（加权平均价格）徘徊在1！看起来物以类聚。\n",
    "- 集体拥抱: 我们对股票进行了分组，所以每组中的股票就像失散多年的兄弟姐妹。它们具有相似的WAP平均值和标准偏差。\n",
    "- 下一步是什么？: 知道哪些股票是好朋友可以很好的帮助我们在特征工程和构建强预测模型。\n",
    "- 有离散: 如果一个群体中的一只股票开始表现得像独狼, 是时候举起红旗了. 可能是一个市场事件或其他值得挖掘的事情！\n",
    "\n",
    "\n",
    "\n",
    "**将类似的股票分组就像组建一支支队伍. 它帮助我们更好地掌握市场动态，并可以完全提升我们的预测模型！**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb3ee0036c1e1cd0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 组中股票的WAP随时间变化\n",
    "# 筛选这十组股票的id\n",
    "stocks_in_first_10_groups = grouped_stocks[grouped_stocks['group'] < 10]['stock_id'].unique()\n",
    "\n",
    "# 创建画布\n",
    "fig, axes = plt.subplots(5, 2, figsize=(20, 30))\n",
    "\n",
    "# 使轴阵列变平，便于索引\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 循环浏览每组，绘制该组股票的WAP\n",
    "for i in range(10):\n",
    "    stocks_in_group = grouped_stocks[grouped_stocks['group'] == i]['stock_id'].unique()\n",
    "    filtered_data = train[train['stock_id'].isin(stocks_in_group)]\n",
    "\n",
    "    sns.lineplot(x='time_id', y='wap', hue='stock_id', data=filtered_data, ax=axes[i])\n",
    "    axes[i].set_title(f'WAP Variation Over Time for Stocks in Group {i + 1}')\n",
    "    axes[i].set_xlabel('Time')\n",
    "    axes[i].set_ylabel('WAP')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90dbd6c843fff2be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "常见模式: 如果一个组合中的多只股票显示出类似的WAP趋势， 代表对配对交易策略有用或多样化.\n",
    "波动性: 观察WAP的起伏，发现过山车. 波动性更大的股票可能会提供更多快速获利的机会, 但别忘了戴帽子，太危险了！\n",
    "分组验证: 情节反复检验我们的“物以类聚”是否真的“群策群力”. 如果没有，返回绘图板"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c80b5c7855031b4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 特征之间的相关性\n",
    "correlation_matrix = train.corr()\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ddc69884e6dfe60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- imbalance_size和matched_size：正相关是有道理的，因为这两个特征都与交易规模有关。正如相关性所反映的那样，更大的失衡确实可能导致匹配股票的规模更大。\n",
    "- reference_price、bid_price、ask_price和wap：这些变量之间的高度相关性是合乎逻辑的，因为它们都与股价有关。具体而言，reference_price是最大股票数量匹配的价格，这自然类似于加权平均价格（wap）和最具竞争力的买卖价格。\n",
    "- imbalance_buy_sell_flag和near_price：相关性表明，失衡的方向（买入或卖出）可能会影响near_prices，near_prince是最大化匹配股票数量的交叉价格之一。这是有道理的，因为买卖不平衡可能会影响大多数股票的交易价格。\n",
    "- seconds_in_bucket和imbalance_size：负相关性很有趣，可能表明随着市场收盘（seconds_in_bucket越高），失衡可能会减少，可能是由于头寸的清算。\n",
    "- target：它与其他变量没有很强的相关性，这表明预测wap的未来变化是复杂的，不能仅仅基于提供的其他变量。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1792609350df9c2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 检查数据集中异常值的百分比\n",
    "# 初始化列表以存储结果\n",
    "outliers_info = []\n",
    "\n",
    "# 计算每个数值列的异常值数量和百分比\n",
    "for col in train.select_dtypes(include=['number']).columns:\n",
    "    Q1 = train[col].quantile(0.25)\n",
    "    Q3 = train[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = train[(train[col] < lower_bound) | (train[col] > upper_bound)]\n",
    "    outliers_quantity = len(outliers)\n",
    "    outliers_percentage = (outliers_quantity / len(train)) * 100\n",
    "    outliers_info.append((col, outliers_quantity, round(outliers_percentage, 2)))\n",
    "\n",
    "outliers_df = pd.DataFrame(outliers_info, columns=['Feature', '异常值数量', '异常值百分比'])\n",
    "\n",
    "# 排序\n",
    "outliers_df = outliers_df.sort_values(by='异常值百分比', ascending=False).reset_index(drop=True)\n",
    "outliers_df = outliers_df[outliers_df['异常值百分比'] > 0]\n",
    "\n",
    "outliers_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72eee5282418d856"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- matched_size和imbalance_size：这些是与交易规模和当前参考价格下不匹配的金额相关的财务指标。此处的异常值可能表示异常市场事件，如大额买入/卖出或价格突然波动。\n",
    "\n",
    "- bid_size、ask_size、far_price、near_price：这些也是财务指标，异常值可能是由于类似的市场事件。调查这些变量的异常值与其他变量（如imbalance_size和matched_size）之间是否存在任何相关性可能会很有趣。\n",
    "\n",
    "- wap、bid_price、ask_price、reference_price和target：这些特征中的异常值也可能是罕见市场事件或波动的症状。\n",
    "\n",
    "- seconds_in_bucket、imbalance_buy_sell_flag、stock_id、date_id、time_id：如前所述，这些是分类或基于时间的变量，没有异常值，这在给定其上下文的情况下是有意义的。\n",
    "\n",
    "结论背后的逻辑：\n",
    "\n",
    "- 背景分析：考虑到我们正在处理金融市场数据，不寻常的事件是意料之中的，可能对预测特别感兴趣。因此，直接去除异常值可能不是所有情况下的最佳策略。\n",
    "\n",
    "- 建模：一些建模技术对异常值更具鲁棒性。根据您为比赛选择的算法，您可能会选择留下异常值或以某种方式处理它们。\n",
    "\n",
    "- 相关性：由于这些变量与市场交易的特定方面有关，了解不同变量中异常值之间的相关性可以深入了解异常市场事件。\n",
    "\n",
    "- 特征工程：您可能希望创建新的变量，以对模型更有用的方式捕获异常值中包含的信息。\n",
    "\n",
    "<font color='yellow'>验证：任何处理异常值的策略都应该经过仔细验证。</font>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0d183d2c6ccdbf4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 研究不同变量中异常值之间的相关性\n",
    "# 识别异常\n",
    "def identify_outliers(df, col):\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "# 识别“matched_size”和“imbalance_size”的异常值\n",
    "outliers_matched_size = identify_outliers(train, 'matched_size')\n",
    "outliers_imbalance_size = identify_outliers(train, 'imbalance_size')\n",
    "\n",
    "# 检查“matched_size”和“imbalance_size”之间是否存在重复行（异常值）\n",
    "common_outliers = pd.merge(outliers_matched_size, outliers_imbalance_size, how='inner')\n",
    "\n",
    "# 计算相关性\n",
    "correlation_matrix = common_outliers.corr()\n",
    "\n",
    "correlation_matrix[['imbalance_size', 'matched_size']].T"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6675695cdbd7206"
  },
  {
   "cell_type": "markdown",
   "source": [
    "考虑到特征描述，结论变得更加相关：\n",
    "- imbalance_size和matched_size之间的适度相关性表明，以当前参考价格（以美元计）不匹配的金额和以当前参考价（以美元为单位）可以匹配的金额在某种程度上是相关的。这可能有助于预测或了解市场行为。\n",
    "- imbalance_size和seconds_in_bucket之间的负相关性可能表明，自当天收盘拍卖开始以来，随着时间的推移，不匹配的数量可能会减少。这可能是一个值得探索的市场现象。\n",
    "- matched_size和seconds_in_bucket之间的正相关性可能表明，随着收盘拍卖的临近，匹配的交易越来越多。\n",
    "- 此外，这些变量与目标之间缺乏显著相关性（股票WAP未来60秒的波动），这表明它们可能不是价格波动的良好直接预测因素，但在更复杂的模型中或与其他变量组合时可能有用。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71201f95be4f1c3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Baseline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42920efec369e36b"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# 优化内存的函数\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage of dataframe is {start_mem:.2f} MB')\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n",
    "    decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "    print(f'Decreased by {decrease:.2f}%')\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:41:49.326550500Z",
     "start_time": "2023-10-25T14:41:49.321549800Z"
    }
   },
   "id": "fe9bda1f4146cc3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import gc\n",
    "# 每个股票的bid_size和ask_size的中位数之和\n",
    "median_vol = train.groupby('stock_id')['bid_size'].median() + train.groupby('stock_id')['ask_size'].median()\n",
    "\n",
    "def feat_eng(df):\n",
    "\n",
    "    cols = [c for c in df.columns if c not in ['row_id', 'time_id']]\n",
    "    df = df[cols]\n",
    "    # 将买入不平衡和卖出不平衡的标志转化为二进制标志\n",
    "    df['imbalance_buy_flag'] = np.where(df['imbalance_buy_sell_flag']==1, 1, 0)\n",
    "    df['imbalance_sell_flag'] = np.where(df['imbalance_buy_sell_flag']==-1, 1, 0)\n",
    "    \n",
    "    # 订单薄中的买单大小和卖单大小之和\n",
    "    df['bid_plus_ask_sizes'] = df['bid_size'] + train['ask_size']\n",
    "    # 每个股票的bid_size和ask_size的中位数之和\n",
    "    df['median_vol'] = df['stock_id'].map(median_vol.to_dict())\n",
    "    # 前订单簿的买卖总量是否高于该股票的median_vol，如果是，则为1，否则为0。\n",
    "    df['high_volume'] = np.where(df['bid_plus_ask_sizes'] > df['median_vol'], 1, 0)\n",
    "    # 不平衡大小（imbalance_size）与匹配大小（matched_size）之间的比率。\n",
    "    df['imbalance_ratio'] = df['imbalance_size'] / df['matched_size']\n",
    "    # 表示买入-卖出不平衡和不平衡大小-匹配大小之间的比率。\n",
    "    df['imb_s1'] = df.eval('(bid_size-ask_size)/(bid_size+ask_size)')\n",
    "    df['imb_s2'] = df.eval('(imbalance_size-matched_size)/(matched_size+imbalance_size)')\n",
    "    # 表示买单和卖单的加权大小，将价格与数量相乘。\n",
    "    df['ask_x_size'] = df.eval('ask_size*ask_price')\n",
    "    df['bid_x_size'] = df.eval('bid_size*bid_price')\n",
    "    # 表示卖单和买单的加权大小之间的差异。\n",
    "    df['ask_minus_bid'] = df['ask_x_size'] - df['bid_x_size']\n",
    "    # 表示买单大小和价格与卖单大小和价格之间的比率。\n",
    "    df[\"bid_size_over_ask_size\"] = df[\"bid_size\"].div(df[\"ask_size\"])\n",
    "    df[\"bid_price_over_ask_price\"] = df[\"bid_price\"].div(df[\"ask_price\"])\n",
    "\n",
    "    prices = ['reference_price','far_price', 'near_price', 'ask_price', 'bid_price', 'wap']\n",
    "    # 各种价格特征的差异、乘积和不平衡比率\n",
    "    for c in combinations(prices, 2):\n",
    "\n",
    "        df[f'{c[0]}_minus_{c[1]}'] = (df[f'{c[0]}'] - df[f'{c[1]}']).astype(np.float32)\n",
    "        df[f'{c[0]}_times_{c[1]}'] = (df[f'{c[0]}'] * df[f'{c[1]}']).astype(np.float32)\n",
    "        df[f'{c[0]}_{c[1]}_imb'] = df.eval(f'({c[0]}-{c[1]})/({c[0]}+{c[1]})')\n",
    "    # 组合三个价格特征的不平衡比率\n",
    "    for c in combinations(prices, 3):\n",
    "        max_ = df[list(c)].max(axis=1)\n",
    "        min_ = df[list(c)].min(axis=1)\n",
    "        mid_ = df[list(c)].sum(axis=1)-min_-max_\n",
    "\n",
    "        df[f'{c[0]}_{c[1]}_{c[2]}_imb2'] = (max_-mid_)/(mid_-min_)\n",
    "    df.drop(columns=['date_id'], inplace=True)\n",
    "\n",
    "\n",
    "    df=reduce_mem_usage(df)\n",
    "    gc.collect()\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:41:49.753551600Z",
     "start_time": "2023-10-25T14:41:49.331558400Z"
    }
   },
   "id": "6e630afa2dda9181"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train.columns"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "970eed76f1e47ec4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- 'imbalance_buy_flag'和 'imbalance_sell_flag'：买入和卖出不平衡标志，用于识别买卖不平衡的方向。\n",
    "\n",
    "- 'bid_plus_ask_sizes'：买单和卖单大小之和，可用于分析市场深度。\n",
    "\n",
    "- 'median_vol'：每个股票的买卖大小中位数，用于分析市场活动。\n",
    "\n",
    "- 'high_volume'：指示当前市场活动是否高于中位数，用于分析高交易量时的行为。\n",
    "\n",
    "- 'imbalance_ratio'：不平衡大小与匹配大小之间的比率，用于分析不平衡情况。\n",
    "\n",
    "- 'imb_s1'和 'imb_s2'：与买卖不平衡相关的衍生特征，用于分析买卖不平衡的趋势。\n",
    "\n",
    "- 'ask_x_size'和 'bid_x_size'：买单和卖单的交易量加权价格，可用于估计市场价格。\n",
    "\n",
    "- 'ask_minus_bid'：卖单和买单的加权价格之间的差异，用于分析价格差异。\n",
    "\n",
    "- 'bid_size_over_ask_size'和 'bid_price_over_ask_price'：买单大小和价格与卖单大小和价格之间的比率，可用于分析价格和交易量之间的关系。\n",
    "\n",
    "- 其他价格特征：包括价格差异、价格变化速度、价格均值和标准差等，用于分析价格的变化和波动性。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c24d31a262920d4"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "train.dropna(subset=['target'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:41:50.396553700Z",
     "start_time": "2023-10-25T14:41:49.770549Z"
    }
   },
   "id": "fa1d22a14d2eb84f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 3017.13 MB\n",
      "Memory usage after optimization is: 1218.84 MB\n",
      "Decreased by 59.60%\n"
     ]
    }
   ],
   "source": [
    "y = train['target']\n",
    "X = feat_eng(train.drop(columns='target'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:42:41.374124700Z",
     "start_time": "2023-10-25T14:41:50.467550100Z"
    }
   },
   "id": "9ecaf6cdc83a13bf"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import lightgbm as lgb\n",
    "# 交叉验证\n",
    "def cross_validate(model, X, y, cv):\n",
    "    scores = np.zeros(cv.n_splits)\n",
    "    for i, (train_index, test_index) in enumerate(cv.split(X)):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, shuffle=False, test_size=0.1)\n",
    "        start = timer()\n",
    "        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "        end = timer()\n",
    "        y_pred = model.predict(X_test)\n",
    "        scores[i] = mean_absolute_error(y_pred, y_test)\n",
    "    return scores"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:42:42.347127700Z",
     "start_time": "2023-10-25T14:42:41.368124500Z"
    }
   },
   "id": "60c86e3b41c1c15d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 评估指标\n",
    "def evaluate_simple(model, X, y, cv):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, shuffle=False, test_size=0.1)\n",
    "    start = timer()\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    end = timer()\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = mean_absolute_error(y_pred, y_test)\n",
    "    return score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:42:42.366124200Z",
     "start_time": "2023-10-25T14:42:42.353127300Z"
    }
   },
   "id": "4753dd91c3b54de1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import optuna\n",
    "import json\n",
    "\n",
    "# 超参数优化的函数\n",
    "## objective:要优化的函数 n_tri：试验的次数 n_jobs:并行册数\n",
    "def run_optimization(objective, n_trials=100, n_jobs=1):\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs, show_progress_bar=True)\n",
    "    with open(\"best_params.json\", \"w\") as f:\n",
    "        json.dump(study.best_params, f)\n",
    "    return study"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:42:42.757127700Z",
     "start_time": "2023-10-25T14:42:42.361124200Z"
    }
   },
   "id": "40ac008fba9251ee"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# 为Optuna超参数优化提供一个目标函数\n",
    "def get_objective_function(evaluation=\"simple\", cv=None, logging_level=\"info\"):\n",
    "    \"\"\"Returns the objective function for optuna.\"\"\"\n",
    "    if evaluation == \"simple\":\n",
    "        eval_function = evaluate_simple\n",
    "    else:\n",
    "        eval_function = cross_validate\n",
    "\n",
    "    def optimize_lgbm(trial):\n",
    "        \"\"\"Optimizes a LGBMRegressor with cross-validation.\"\"\"\n",
    "        # num_leaves should be smaller than 2^{max_depth}\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 6, 9)\n",
    "        num_leaves = trial.suggest_int(\"num_leaves\", 32, int((2**max_depth) * 0.90))\n",
    "\n",
    "        param_space = {\n",
    "            \"boosting\":'gbdt',\n",
    "            \"objective\": trial.suggest_categorical(\"objective\", [\"mae\"]),\n",
    "            \"random_state\": trial.suggest_categorical(\"random_state\", [42]),\n",
    "            \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [600]),\n",
    "            \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-3, 1.0, log=True),\n",
    "            \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-3, 1.0, log=True),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-2, 2e-1, log=True),\n",
    "            \"num_leaves\": num_leaves,\n",
    "            \"max_depth\": max_depth\n",
    "        }\n",
    "        model = LGBMRegressor(**param_space)\n",
    "        scores = eval_function(model, X, y, cv=cv)\n",
    "        return scores.mean()\n",
    "    return optimize_lgbm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:42:42.770123600Z",
     "start_time": "2023-10-25T14:42:42.760122600Z"
    }
   },
   "id": "42fe8ce62e0bd92a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m = lgb.LGBMRegressor(objective='mae', n_estimators=600, random_state=51)\n",
    "m.fit(X, y)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-25T14:42:42.764123600Z"
    }
   },
   "id": "d55a3490ae62b5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "feat_imp = pd.Series(m.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "print('Columns with poor contribution', feat_imp[feat_imp<10].index)\n",
    "fig = px.bar(x=feat_imp, y=feat_imp.index, orientation='h')\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11fbeb1e1a6d927f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test=feat_eng(test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52d3199a14bc4584"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 开始优化\n",
    "\n",
    "可以设置以下参数进行模型优化：\n",
    "\n",
    "- run_lgbm_optimization：是否运行优化或使用已计算的优化。\n",
    "- n_trials：我们想要采样多少次试验。\n",
    "- logging_level：配置评估函数内的日志记录级别（使用“info”或“success”）\n",
    "- 评估：使用“simple”进行简单的训练-测试分割，或使用“cross_validate”使用 TimeSeriesSplit 进行交叉验证。\n",
    "- cv：分割对象\n",
    "- 警告：evaluation='cross_validate'需要很长时间！"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "960102d5b1832cdb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gc.collect()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d84ff129267270c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_lgbm_optimization = True\n",
    "n_trials = 30\n",
    "logging_level = \"success\"\n",
    "evaluation = \"simple\"\n",
    "cv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from optuna.visualization import (\n",
    "    plot_optimization_history,\n",
    "    plot_param_importances,\n",
    "    plot_parallel_coordinate,\n",
    ")\n",
    "\n",
    "if run_lgbm_optimization:\n",
    "    clear_output(wait=True) \n",
    "    objective = get_objective_function(evaluation=evaluation, cv=cv)\n",
    "    study = run_optimization(objective, n_trials=n_trials, n_jobs=1)\n",
    "    lgbm_best_params = study.best_params\n",
    "\n",
    "    plot_optimization_history(study).show()\n",
    "    if n_trials > 1:\n",
    "        plot_param_importances(study).show()\n",
    "        plot_parallel_coordinate(study).show()\n",
    "else:\n",
    "    lgbm_best_params = {}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "beea67df0c1a178c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型推理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dad046e78367779"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 如果你优化了，请重新训练并保存\n",
    "# model = LGBMRegressor(**lgbm_best_params)\n",
    "# model.fit(X,y)\n",
    "\n",
    "m.predict(test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e691ff3aa8843cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 提交"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8778c6e6482cd6be"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# kaggle\n",
    "# import optiver2023\n",
    "# env = optiver2023.make_env()\n",
    "# iter_test = env.iter_test()\n",
    "\n",
    "# local\n",
    "from public_timeseries_testing_util import MockApi\n",
    "env=MockApi()\n",
    "iter_test = env.iter_test()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9de9437cfc505663"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83e76b6173c8fef9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def zero_sum(prices, volumes):\n",
    "\n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices)/np.sum(std_error)\n",
    "    out = prices-std_error*step\n",
    "\n",
    "    return out"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4db8d242a8e5fcdd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "\n",
    "    feat = feat_eng(test)\n",
    "    sample_prediction['target'] = m.predict(feat)\n",
    "\n",
    "    sample_prediction['target'] = zero_sum(sample_prediction['target'], test.loc[:,'bid_size'] + test.loc[:,'ask_size'])\n",
    "\n",
    "    env.predict(sample_prediction)\n",
    "\n",
    "    counter += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32180df254fecd4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 模型保存\n",
    "import joblib\n",
    "joblib.dump(m,'./baseline1_model')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-25T14:41:28.308378700Z",
     "start_time": "2023-10-25T14:41:28.305377Z"
    }
   },
   "id": "852cf4ecccfbc142"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2ecdd92f39ed7109"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
